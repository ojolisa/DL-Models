{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":96480,"databundleVersionId":11466546,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:30:25.601820Z","iopub.execute_input":"2025-03-29T07:30:25.602060Z","iopub.status.idle":"2025-03-29T07:30:31.702091Z","shell.execute_reply.started":"2025-03-29T07:30:25.602039Z","shell.execute_reply":"2025-03-29T07:30:31.701458Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class DepthEstimationDataset(Dataset):\n    def __init__(self, images_dir, depths_dir=None, transform=None):\n        \"\"\"\n        Dataset for depth estimation with optional depth maps\n        \n        Args:\n            images_dir (str): Directory containing images\n            depths_dir (str, optional): Directory containing depth maps\n            transform (callable, optional): Optional transform to be applied on images\n        \"\"\"\n        self.images_dir = images_dir\n        self.depths_dir = depths_dir\n        self.transform = transform\n        \n        # Get list of image filenames\n        self.image_filenames = sorted(os.listdir(images_dir))\n        \n        # If depth directory is provided, get depth filenames\n        self.depth_filenames = sorted(os.listdir(depths_dir)) if depths_dir else None\n    \n    def __len__(self):\n        return len(self.image_filenames)\n    \n    def __getitem__(self, idx):\n        # Load image\n        img_path = os.path.join(self.images_dir, self.image_filenames[idx])\n        image = Image.open(img_path).convert('RGB')\n    \n        # Load depth map if available\n        if self.depth_filenames:\n            depth_path = os.path.join(self.depths_dir, self.depth_filenames[idx])\n            depth = Image.open(depth_path).convert('L')  # Convert depth to grayscale\n            \n            if self.transform:\n                image = self.transform(image)\n                depth = transforms.ToTensor()(depth)  # Convert depth to a single-channel tensor\n    \n            return image, depth\n        else:\n            if self.transform:\n                image = self.transform(image)\n    \n            return image, self.image_filenames[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:30:31.702711Z","iopub.execute_input":"2025-03-29T07:30:31.703008Z","iopub.status.idle":"2025-03-29T07:30:31.709221Z","shell.execute_reply.started":"2025-03-29T07:30:31.702990Z","shell.execute_reply":"2025-03-29T07:30:31.708395Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class DepthEstimationModel(nn.Module):\n    def __init__(self, pretrained=True):\n        \"\"\"\n        Advanced Depth Estimation Model using ResNet as encoder\n        \n        Args:\n            pretrained (bool): Use pretrained weights for encoder\n        \"\"\"\n        super(DepthEstimationModel, self).__init__()\n        \n        # Use ResNet50 as encoder backbone\n        backbone = torchvision.models.resnet50(pretrained=pretrained)\n        \n        # Encoder layers (first 4 blocks of ResNet)\n        self.encoder1 = nn.Sequential(\n            backbone.conv1,\n            backbone.bn1,\n            backbone.relu,\n            backbone.maxpool\n        )\n        self.encoder2 = backbone.layer1\n        self.encoder3 = backbone.layer2\n        self.encoder4 = backbone.layer3\n        self.encoder5 = backbone.layer4\n        \n        # Decoder layers with skip connections\n        self.decoder5 = self._upconv_block(2048, 1024)\n        self.decoder4 = self._upconv_block(1024, 512)\n        self.decoder3 = self._upconv_block(512, 256)\n        self.decoder2 = self._upconv_block(256, 64)\n        self.decoder1 = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True)\n        )\n        \n        # Final depth prediction layer\n        self.final_conv = nn.Conv2d(32, 1, kernel_size=3, padding=1)\n        \n    def _upconv_block(self, in_channels, out_channels):\n        \"\"\"\n        Create an upconvolution block with skip connection\n        \"\"\"\n        return nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass through the depth estimation network\n        \"\"\"\n        # Encoder pass\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n        e5 = self.encoder5(e4)\n        \n        # Decoder pass with skip connections\n        d5 = self.decoder5(e5)\n        d4 = self.decoder4(d5)\n        d3 = self.decoder3(d4)\n        d2 = self.decoder2(d3)\n        d1 = self.decoder1(d2)\n        \n        # Final depth prediction\n        depth = self.final_conv(d1)\n        \n        return depth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:30:31.710064Z","iopub.execute_input":"2025-03-29T07:30:31.710352Z","iopub.status.idle":"2025-03-29T07:30:31.724772Z","shell.execute_reply.started":"2025-03-29T07:30:31.710325Z","shell.execute_reply":"2025-03-29T07:30:31.723960Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n    \"\"\"\n    Train the depth estimation model with progress updates.\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    train_losses, val_losses = [], []\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_epoch_loss = 0.0\n\n        print(f\"\\nEpoch {epoch + 1}/{num_epochs} --------------------------\")\n\n        for batch_idx, (images, depths) in enumerate(train_loader):\n            images, depths = images.to(device), depths.to(device)\n\n            optimizer.zero_grad()\n            predicted_depths = model(images)\n\n            loss = criterion(predicted_depths, depths)\n            loss.backward()\n            optimizer.step()\n\n            train_epoch_loss += loss.item()\n\n            # Print batch progress every 10 batches\n            if (batch_idx + 1) % 10 == 0 or batch_idx == len(train_loader) - 1:\n                print(\n                    f\"Batch {batch_idx + 1}/{len(train_loader)} - Train Loss: {loss.item():.6f}\"\n                )\n\n        # Validation phase\n        model.eval()\n        val_epoch_loss = 0.0\n\n        with torch.no_grad():\n            for images, depths in val_loader:\n                images, depths = images.to(device), depths.to(device)\n                predicted_depths = model(images)\n\n                loss = criterion(predicted_depths, depths)\n                val_epoch_loss += loss.item()\n\n        train_loss = train_epoch_loss / len(train_loader)\n        val_loss = val_epoch_loss / len(val_loader)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n\n        # Print epoch-level progress\n        print(f\"Epoch {epoch + 1} Completed ✅ | Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n\n    return {\"train_losses\": train_losses, \"val_losses\": val_losses}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:30:31.726694Z","iopub.execute_input":"2025-03-29T07:30:31.726921Z","iopub.status.idle":"2025-03-29T07:30:31.739314Z","shell.execute_reply.started":"2025-03-29T07:30:31.726892Z","shell.execute_reply":"2025-03-29T07:30:31.738560Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def generate_predictions(model, test_loader):\n    \"\"\"\n    Generate predictions on test dataset\n    \"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    model.eval()\n    \n    predictions = []\n    image_names = []\n    \n    with torch.no_grad():\n        for images, filenames in test_loader:\n            images = images.to(device)\n            predicted_depths = model(images)\n            \n            # Convert predictions to numpy\n            predicted_depths_np = predicted_depths.cpu().numpy()\n            \n            # Resize and normalize\n            processed_depths = []\n            for pred in predicted_depths_np:\n                # Remove singleton dimensions and reshape\n                pred = pred.squeeze()\n                \n                # Resize to 128x128\n                pred_resized = cv2.resize(pred, (128, 128))\n                \n                # Normalize to 0-1 range\n                pred_norm = (pred_resized - pred_resized.min()) / (pred_resized.max() - pred_resized.min() + 1e-6)\n                \n                # Convert to 8-bit image\n                pred_8bit = np.uint8(pred_norm * 255)\n                \n                processed_depths.append(pred_8bit)\n            \n            predictions.extend(processed_depths)\n            image_names.extend(filenames)\n    \n    return predictions, image_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:30:31.740449Z","iopub.execute_input":"2025-03-29T07:30:31.740714Z","iopub.status.idle":"2025-03-29T07:30:31.753925Z","shell.execute_reply.started":"2025-03-29T07:30:31.740684Z","shell.execute_reply":"2025-03-29T07:30:31.753252Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def images_to_csv_with_metadata(predictions, image_names, output_csv='predictions.csv'):\n    \"\"\"\n    Convert depth predictions to CSV with metadata\n    \"\"\"\n    data = []\n    for idx, (pred, filename) in enumerate(zip(predictions, image_names)):\n        # Flatten the image into a 1D array\n        image_flat = pred.flatten()\n        \n        # Create row with ID, ImageID, and pixel values\n        row = [idx, filename] + image_flat.tolist()\n        data.append(row)\n    \n    # Create column names\n    num_columns = len(data[0]) - 2 if data else 0\n    column_names = [\"id\", \"ImageID\"] + [indx for indx in range(num_columns)]\n    \n    # Create DataFrame and save to CSV\n    df = pd.DataFrame(data, columns=column_names)\n    df.to_csv(output_csv, index=False)\n    print(f\"Predictions saved to {output_csv}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:30:31.754521Z","iopub.execute_input":"2025-03-29T07:30:31.754700Z","iopub.status.idle":"2025-03-29T07:30:31.768490Z","shell.execute_reply.started":"2025-03-29T07:30:31.754684Z","shell.execute_reply":"2025-03-29T07:30:31.767672Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"torch.manual_seed(42)\nnp.random.seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:30:31.769139Z","iopub.execute_input":"2025-03-29T07:30:31.769378Z","iopub.status.idle":"2025-03-29T07:30:31.786029Z","shell.execute_reply.started":"2025-03-29T07:30:31.769360Z","shell.execute_reply":"2025-03-29T07:30:31.785334Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Data transforms\ntransforms_config = {\n    'train': transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        )\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        )\n    ])\n}\n    \n# Create datasets\ntrain_dataset = DepthEstimationDataset(\n    '/kaggle/input/depth-estimation/competition-data/competition-data/training/images', \n    '/kaggle/input/depth-estimation/competition-data/competition-data/training/depths', \n    transform=transforms_config['train']\n)\n\nval_dataset = DepthEstimationDataset(\n    '/kaggle/input/depth-estimation/competition-data/competition-data/validation/images', \n    '/kaggle/input/depth-estimation/competition-data/competition-data/validation/depths', \n    transform=transforms_config['test']\n)\n\ntest_dataset = DepthEstimationDataset(\n    '/kaggle/input/depth-estimation/competition-data/competition-data/testing/images', \n    transform=transforms_config['test']\n)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:30:31.786829Z","iopub.execute_input":"2025-03-29T07:30:31.787054Z","iopub.status.idle":"2025-03-29T07:30:31.951660Z","shell.execute_reply.started":"2025-03-29T07:30:31.787036Z","shell.execute_reply":"2025-03-29T07:30:31.950868Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Initialize model, loss, and optimizer\nmodel = DepthEstimationModel()\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:30:31.952367Z","iopub.execute_input":"2025-03-29T07:30:31.952602Z","iopub.status.idle":"2025-03-29T07:30:33.346929Z","shell.execute_reply.started":"2025-03-29T07:30:31.952583Z","shell.execute_reply":"2025-03-29T07:30:33.346260Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 181MB/s] \n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Train the model\ntraining_results = train_model(\n    model, \n    train_loader, \n    val_loader, \n    criterion, \n    optimizer, \n    num_epochs=40\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:30:33.347752Z","iopub.execute_input":"2025-03-29T07:30:33.347990Z","iopub.status.idle":"2025-03-29T10:44:51.293888Z","shell.execute_reply.started":"2025-03-29T07:30:33.347971Z","shell.execute_reply":"2025-03-29T10:44:51.293168Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/40 --------------------------\nBatch 10/209 - Train Loss: 0.048451\nBatch 20/209 - Train Loss: 0.023084\nBatch 30/209 - Train Loss: 0.019104\nBatch 40/209 - Train Loss: 0.016305\nBatch 50/209 - Train Loss: 0.018525\nBatch 60/209 - Train Loss: 0.018527\nBatch 70/209 - Train Loss: 0.014867\nBatch 80/209 - Train Loss: 0.015225\nBatch 90/209 - Train Loss: 0.012472\nBatch 100/209 - Train Loss: 0.014817\nBatch 110/209 - Train Loss: 0.011282\nBatch 120/209 - Train Loss: 0.014483\nBatch 130/209 - Train Loss: 0.020324\nBatch 140/209 - Train Loss: 0.011165\nBatch 150/209 - Train Loss: 0.011487\nBatch 160/209 - Train Loss: 0.013081\nBatch 170/209 - Train Loss: 0.011302\nBatch 180/209 - Train Loss: 0.010401\nBatch 190/209 - Train Loss: 0.014779\nBatch 200/209 - Train Loss: 0.010706\nBatch 209/209 - Train Loss: 0.012276\nEpoch 1 Completed ✅ | Training Loss: 0.0207, Validation Loss: 0.0111\n\nEpoch 2/40 --------------------------\nBatch 10/209 - Train Loss: 0.011949\nBatch 20/209 - Train Loss: 0.010059\nBatch 30/209 - Train Loss: 0.011137\nBatch 40/209 - Train Loss: 0.011763\nBatch 50/209 - Train Loss: 0.011097\nBatch 60/209 - Train Loss: 0.012416\nBatch 70/209 - Train Loss: 0.012277\nBatch 80/209 - Train Loss: 0.009555\nBatch 90/209 - Train Loss: 0.010579\nBatch 100/209 - Train Loss: 0.010790\nBatch 110/209 - Train Loss: 0.007570\nBatch 120/209 - Train Loss: 0.010891\nBatch 130/209 - Train Loss: 0.010697\nBatch 140/209 - Train Loss: 0.010565\nBatch 150/209 - Train Loss: 0.008422\nBatch 160/209 - Train Loss: 0.010929\nBatch 170/209 - Train Loss: 0.006662\nBatch 180/209 - Train Loss: 0.008159\nBatch 190/209 - Train Loss: 0.012595\nBatch 200/209 - Train Loss: 0.009375\nBatch 209/209 - Train Loss: 0.006118\nEpoch 2 Completed ✅ | Training Loss: 0.0110, Validation Loss: 0.0094\n\nEpoch 3/40 --------------------------\nBatch 10/209 - Train Loss: 0.013984\nBatch 20/209 - Train Loss: 0.009942\nBatch 30/209 - Train Loss: 0.006972\nBatch 40/209 - Train Loss: 0.010427\nBatch 50/209 - Train Loss: 0.008976\nBatch 60/209 - Train Loss: 0.011328\nBatch 70/209 - Train Loss: 0.006979\nBatch 80/209 - Train Loss: 0.009493\nBatch 90/209 - Train Loss: 0.013454\nBatch 100/209 - Train Loss: 0.008816\nBatch 110/209 - Train Loss: 0.011744\nBatch 120/209 - Train Loss: 0.007103\nBatch 130/209 - Train Loss: 0.009252\nBatch 140/209 - Train Loss: 0.007759\nBatch 150/209 - Train Loss: 0.007709\nBatch 160/209 - Train Loss: 0.009306\nBatch 170/209 - Train Loss: 0.012659\nBatch 180/209 - Train Loss: 0.008950\nBatch 190/209 - Train Loss: 0.007364\nBatch 200/209 - Train Loss: 0.008675\nBatch 209/209 - Train Loss: 0.007117\nEpoch 3 Completed ✅ | Training Loss: 0.0093, Validation Loss: 0.0094\n\nEpoch 4/40 --------------------------\nBatch 10/209 - Train Loss: 0.007214\nBatch 20/209 - Train Loss: 0.006891\nBatch 30/209 - Train Loss: 0.005910\nBatch 40/209 - Train Loss: 0.006100\nBatch 50/209 - Train Loss: 0.008705\nBatch 60/209 - Train Loss: 0.007134\nBatch 70/209 - Train Loss: 0.009110\nBatch 80/209 - Train Loss: 0.006429\nBatch 90/209 - Train Loss: 0.007623\nBatch 100/209 - Train Loss: 0.007853\nBatch 110/209 - Train Loss: 0.008579\nBatch 120/209 - Train Loss: 0.008319\nBatch 130/209 - Train Loss: 0.006780\nBatch 140/209 - Train Loss: 0.005622\nBatch 150/209 - Train Loss: 0.007557\nBatch 160/209 - Train Loss: 0.006669\nBatch 170/209 - Train Loss: 0.009187\nBatch 180/209 - Train Loss: 0.009284\nBatch 200/209 - Train Loss: 0.008460\nBatch 209/209 - Train Loss: 0.006295\nEpoch 4 Completed ✅ | Training Loss: 0.0081, Validation Loss: 0.0091\n\nEpoch 5/40 --------------------------\nBatch 10/209 - Train Loss: 0.005950\nBatch 20/209 - Train Loss: 0.006845\nBatch 30/209 - Train Loss: 0.006872\nBatch 40/209 - Train Loss: 0.006552\nBatch 50/209 - Train Loss: 0.006206\nBatch 60/209 - Train Loss: 0.008595\nBatch 70/209 - Train Loss: 0.004673\nBatch 80/209 - Train Loss: 0.008718\nBatch 90/209 - Train Loss: 0.006126\nBatch 100/209 - Train Loss: 0.005610\nBatch 110/209 - Train Loss: 0.008502\nBatch 120/209 - Train Loss: 0.006813\nBatch 130/209 - Train Loss: 0.005470\nBatch 140/209 - Train Loss: 0.008238\nBatch 150/209 - Train Loss: 0.005755\nBatch 160/209 - Train Loss: 0.006943\nBatch 170/209 - Train Loss: 0.005108\nBatch 180/209 - Train Loss: 0.008095\nBatch 190/209 - Train Loss: 0.007840\nBatch 200/209 - Train Loss: 0.008088\nBatch 209/209 - Train Loss: 0.005429\nEpoch 5 Completed ✅ | Training Loss: 0.0070, Validation Loss: 0.0086\n\nEpoch 6/40 --------------------------\nBatch 10/209 - Train Loss: 0.008928\nBatch 20/209 - Train Loss: 0.007152\nBatch 30/209 - Train Loss: 0.005657\nBatch 40/209 - Train Loss: 0.005682\nBatch 50/209 - Train Loss: 0.005526\nBatch 60/209 - Train Loss: 0.003967\nBatch 70/209 - Train Loss: 0.005147\nBatch 80/209 - Train Loss: 0.005642\nBatch 90/209 - Train Loss: 0.008023\nBatch 100/209 - Train Loss: 0.007185\nBatch 110/209 - Train Loss: 0.005607\nBatch 120/209 - Train Loss: 0.004923\nBatch 130/209 - Train Loss: 0.006812\nBatch 140/209 - Train Loss: 0.006893\nBatch 150/209 - Train Loss: 0.008294\nBatch 160/209 - Train Loss: 0.005788\nBatch 170/209 - Train Loss: 0.005764\nBatch 180/209 - Train Loss: 0.004866\nBatch 190/209 - Train Loss: 0.008090\nBatch 200/209 - Train Loss: 0.007232\nBatch 209/209 - Train Loss: 0.005838\nEpoch 6 Completed ✅ | Training Loss: 0.0062, Validation Loss: 0.0084\n\nEpoch 7/40 --------------------------\nBatch 10/209 - Train Loss: 0.004763\nBatch 20/209 - Train Loss: 0.004792\nBatch 30/209 - Train Loss: 0.005577\nBatch 40/209 - Train Loss: 0.005682\nBatch 50/209 - Train Loss: 0.005495\nBatch 60/209 - Train Loss: 0.009966\nBatch 70/209 - Train Loss: 0.007134\nBatch 80/209 - Train Loss: 0.007221\nBatch 90/209 - Train Loss: 0.005616\nBatch 100/209 - Train Loss: 0.005135\nBatch 110/209 - Train Loss: 0.005824\nBatch 120/209 - Train Loss: 0.006867\nBatch 130/209 - Train Loss: 0.005651\nBatch 140/209 - Train Loss: 0.004921\nBatch 150/209 - Train Loss: 0.005590\nBatch 160/209 - Train Loss: 0.004316\nBatch 170/209 - Train Loss: 0.007114\nBatch 180/209 - Train Loss: 0.005839\nBatch 190/209 - Train Loss: 0.005528\nBatch 200/209 - Train Loss: 0.004530\nBatch 209/209 - Train Loss: 0.005368\nEpoch 7 Completed ✅ | Training Loss: 0.0056, Validation Loss: 0.0086\n\nEpoch 8/40 --------------------------\nBatch 10/209 - Train Loss: 0.003744\nBatch 20/209 - Train Loss: 0.004109\nBatch 30/209 - Train Loss: 0.004629\nBatch 40/209 - Train Loss: 0.005607\nBatch 50/209 - Train Loss: 0.004733\nBatch 60/209 - Train Loss: 0.003938\nBatch 70/209 - Train Loss: 0.004434\nBatch 80/209 - Train Loss: 0.004419\nBatch 90/209 - Train Loss: 0.004892\nBatch 100/209 - Train Loss: 0.005800\nBatch 110/209 - Train Loss: 0.003597\nBatch 120/209 - Train Loss: 0.005800\nBatch 130/209 - Train Loss: 0.004727\nBatch 140/209 - Train Loss: 0.003891\nBatch 150/209 - Train Loss: 0.004310\nBatch 160/209 - Train Loss: 0.005349\nBatch 170/209 - Train Loss: 0.004752\nBatch 180/209 - Train Loss: 0.004412\nBatch 190/209 - Train Loss: 0.007731\nBatch 200/209 - Train Loss: 0.004309\nBatch 209/209 - Train Loss: 0.003643\nEpoch 8 Completed ✅ | Training Loss: 0.0050, Validation Loss: 0.0076\n\nEpoch 9/40 --------------------------\nBatch 10/209 - Train Loss: 0.003788\nBatch 20/209 - Train Loss: 0.004802\nBatch 30/209 - Train Loss: 0.004523\nBatch 40/209 - Train Loss: 0.004715\nBatch 50/209 - Train Loss: 0.005216\nBatch 60/209 - Train Loss: 0.006077\nBatch 70/209 - Train Loss: 0.003177\nBatch 80/209 - Train Loss: 0.008552\nBatch 90/209 - Train Loss: 0.005293\nBatch 100/209 - Train Loss: 0.004172\nBatch 110/209 - Train Loss: 0.003623\nBatch 120/209 - Train Loss: 0.005416\nBatch 130/209 - Train Loss: 0.005153\nBatch 140/209 - Train Loss: 0.005296\nBatch 150/209 - Train Loss: 0.004483\nBatch 160/209 - Train Loss: 0.004617\nBatch 170/209 - Train Loss: 0.004109\nBatch 180/209 - Train Loss: 0.004921\nBatch 190/209 - Train Loss: 0.003917\nBatch 200/209 - Train Loss: 0.004598\nBatch 209/209 - Train Loss: 0.005897\nEpoch 9 Completed ✅ | Training Loss: 0.0047, Validation Loss: 0.0075\n\nEpoch 10/40 --------------------------\nBatch 10/209 - Train Loss: 0.005078\nBatch 20/209 - Train Loss: 0.005239\nBatch 30/209 - Train Loss: 0.004319\nBatch 40/209 - Train Loss: 0.005123\nBatch 50/209 - Train Loss: 0.004474\nBatch 60/209 - Train Loss: 0.003846\nBatch 70/209 - Train Loss: 0.002634\nBatch 80/209 - Train Loss: 0.003263\nBatch 90/209 - Train Loss: 0.005090\nBatch 100/209 - Train Loss: 0.003220\nBatch 110/209 - Train Loss: 0.003123\nBatch 120/209 - Train Loss: 0.004734\nBatch 130/209 - Train Loss: 0.003793\nBatch 140/209 - Train Loss: 0.004291\nBatch 150/209 - Train Loss: 0.004484\nBatch 160/209 - Train Loss: 0.003958\nBatch 170/209 - Train Loss: 0.003640\nBatch 180/209 - Train Loss: 0.003589\nBatch 190/209 - Train Loss: 0.005364\nBatch 200/209 - Train Loss: 0.004875\nBatch 209/209 - Train Loss: 0.004004\nEpoch 10 Completed ✅ | Training Loss: 0.0044, Validation Loss: 0.0076\n\nEpoch 11/40 --------------------------\nBatch 10/209 - Train Loss: 0.003875\nBatch 20/209 - Train Loss: 0.003411\nBatch 30/209 - Train Loss: 0.004145\nBatch 40/209 - Train Loss: 0.003814\nBatch 50/209 - Train Loss: 0.003651\nBatch 60/209 - Train Loss: 0.002878\nBatch 70/209 - Train Loss: 0.003439\nBatch 80/209 - Train Loss: 0.004418\nBatch 90/209 - Train Loss: 0.004286\nBatch 100/209 - Train Loss: 0.003496\nBatch 110/209 - Train Loss: 0.003217\nBatch 120/209 - Train Loss: 0.003772\nBatch 130/209 - Train Loss: 0.003054\nBatch 140/209 - Train Loss: 0.004785\nBatch 150/209 - Train Loss: 0.003466\nBatch 160/209 - Train Loss: 0.003394\nBatch 170/209 - Train Loss: 0.003509\nBatch 180/209 - Train Loss: 0.002885\nBatch 190/209 - Train Loss: 0.004008\nBatch 200/209 - Train Loss: 0.004445\nBatch 209/209 - Train Loss: 0.004133\nEpoch 11 Completed ✅ | Training Loss: 0.0041, Validation Loss: 0.0072\n\nEpoch 12/40 --------------------------\nBatch 10/209 - Train Loss: 0.004434\nBatch 20/209 - Train Loss: 0.003851\nBatch 30/209 - Train Loss: 0.004819\nBatch 40/209 - Train Loss: 0.004330\nBatch 50/209 - Train Loss: 0.003770\nBatch 60/209 - Train Loss: 0.003372\nBatch 70/209 - Train Loss: 0.003878\nBatch 80/209 - Train Loss: 0.003266\nBatch 90/209 - Train Loss: 0.003745\nBatch 100/209 - Train Loss: 0.002824\nBatch 110/209 - Train Loss: 0.004582\nBatch 120/209 - Train Loss: 0.003674\nBatch 130/209 - Train Loss: 0.004149\nBatch 140/209 - Train Loss: 0.003117\nBatch 150/209 - Train Loss: 0.003988\nBatch 160/209 - Train Loss: 0.004861\nBatch 170/209 - Train Loss: 0.003818\nBatch 180/209 - Train Loss: 0.002931\nBatch 190/209 - Train Loss: 0.004018\nBatch 200/209 - Train Loss: 0.005043\nBatch 209/209 - Train Loss: 0.004641\nEpoch 12 Completed ✅ | Training Loss: 0.0038, Validation Loss: 0.0069\n\nEpoch 13/40 --------------------------\nBatch 10/209 - Train Loss: 0.002831\nBatch 20/209 - Train Loss: 0.003061\nBatch 30/209 - Train Loss: 0.004130\nBatch 40/209 - Train Loss: 0.003534\nBatch 50/209 - Train Loss: 0.005926\nBatch 60/209 - Train Loss: 0.003762\nBatch 70/209 - Train Loss: 0.002857\nBatch 80/209 - Train Loss: 0.003388\nBatch 90/209 - Train Loss: 0.004254\nBatch 100/209 - Train Loss: 0.003324\nBatch 110/209 - Train Loss: 0.002875\nBatch 120/209 - Train Loss: 0.003703\nBatch 130/209 - Train Loss: 0.003712\nBatch 140/209 - Train Loss: 0.002881\nBatch 150/209 - Train Loss: 0.003712\nBatch 160/209 - Train Loss: 0.003010\nBatch 170/209 - Train Loss: 0.003858\nBatch 180/209 - Train Loss: 0.003603\nBatch 190/209 - Train Loss: 0.003300\nBatch 200/209 - Train Loss: 0.003813\nBatch 209/209 - Train Loss: 0.003242\nEpoch 13 Completed ✅ | Training Loss: 0.0036, Validation Loss: 0.0068\n\nEpoch 14/40 --------------------------\nBatch 10/209 - Train Loss: 0.003812\nBatch 20/209 - Train Loss: 0.002174\nBatch 30/209 - Train Loss: 0.002981\nBatch 40/209 - Train Loss: 0.003808\nBatch 50/209 - Train Loss: 0.002769\nBatch 60/209 - Train Loss: 0.003644\nBatch 70/209 - Train Loss: 0.003375\nBatch 80/209 - Train Loss: 0.002952\nBatch 90/209 - Train Loss: 0.002718\nBatch 100/209 - Train Loss: 0.002916\nBatch 110/209 - Train Loss: 0.002638\nBatch 120/209 - Train Loss: 0.003021\nBatch 130/209 - Train Loss: 0.003988\nBatch 140/209 - Train Loss: 0.004033\nBatch 150/209 - Train Loss: 0.003287\nBatch 160/209 - Train Loss: 0.003028\nBatch 170/209 - Train Loss: 0.002866\nBatch 180/209 - Train Loss: 0.002971\nBatch 190/209 - Train Loss: 0.003729\nBatch 200/209 - Train Loss: 0.003500\nBatch 209/209 - Train Loss: 0.003484\nEpoch 14 Completed ✅ | Training Loss: 0.0034, Validation Loss: 0.0067\n\nEpoch 15/40 --------------------------\nBatch 10/209 - Train Loss: 0.003316\nBatch 20/209 - Train Loss: 0.003625\nBatch 30/209 - Train Loss: 0.003305\nBatch 40/209 - Train Loss: 0.003480\nBatch 50/209 - Train Loss: 0.003038\nBatch 60/209 - Train Loss: 0.003111\nBatch 70/209 - Train Loss: 0.003251\nBatch 80/209 - Train Loss: 0.003672\nBatch 90/209 - Train Loss: 0.002501\nBatch 100/209 - Train Loss: 0.004724\nBatch 110/209 - Train Loss: 0.003802\nBatch 120/209 - Train Loss: 0.004628\nBatch 130/209 - Train Loss: 0.002731\nBatch 140/209 - Train Loss: 0.002684\nBatch 150/209 - Train Loss: 0.004698\nBatch 160/209 - Train Loss: 0.003333\nBatch 170/209 - Train Loss: 0.003861\nBatch 180/209 - Train Loss: 0.002946\nBatch 190/209 - Train Loss: 0.002979\nBatch 200/209 - Train Loss: 0.003259\nBatch 209/209 - Train Loss: 0.002915\nEpoch 15 Completed ✅ | Training Loss: 0.0033, Validation Loss: 0.0068\n\nEpoch 16/40 --------------------------\nBatch 10/209 - Train Loss: 0.002794\nBatch 20/209 - Train Loss: 0.003143\nBatch 30/209 - Train Loss: 0.002746\nBatch 40/209 - Train Loss: 0.003725\nBatch 50/209 - Train Loss: 0.002543\nBatch 60/209 - Train Loss: 0.002819\nBatch 70/209 - Train Loss: 0.002910\nBatch 80/209 - Train Loss: 0.003098\nBatch 90/209 - Train Loss: 0.002950\nBatch 100/209 - Train Loss: 0.003893\nBatch 110/209 - Train Loss: 0.003768\nBatch 120/209 - Train Loss: 0.004143\nBatch 130/209 - Train Loss: 0.004578\nBatch 140/209 - Train Loss: 0.003243\nBatch 150/209 - Train Loss: 0.002602\nBatch 160/209 - Train Loss: 0.003036\nBatch 170/209 - Train Loss: 0.003078\nBatch 180/209 - Train Loss: 0.003291\nBatch 190/209 - Train Loss: 0.002645\nBatch 200/209 - Train Loss: 0.002815\nBatch 209/209 - Train Loss: 0.003564\nEpoch 16 Completed ✅ | Training Loss: 0.0032, Validation Loss: 0.0068\n\nEpoch 17/40 --------------------------\nBatch 10/209 - Train Loss: 0.003216\nBatch 20/209 - Train Loss: 0.004702\nBatch 30/209 - Train Loss: 0.003510\nBatch 40/209 - Train Loss: 0.002269\nBatch 50/209 - Train Loss: 0.002635\nBatch 60/209 - Train Loss: 0.003734\nBatch 70/209 - Train Loss: 0.003339\nBatch 80/209 - Train Loss: 0.003474\nBatch 90/209 - Train Loss: 0.002885\nBatch 100/209 - Train Loss: 0.003312\nBatch 110/209 - Train Loss: 0.002737\nBatch 120/209 - Train Loss: 0.003140\nBatch 130/209 - Train Loss: 0.002170\nBatch 140/209 - Train Loss: 0.003062\nBatch 150/209 - Train Loss: 0.004735\nBatch 160/209 - Train Loss: 0.003302\nBatch 170/209 - Train Loss: 0.002903\nBatch 180/209 - Train Loss: 0.003565\nBatch 190/209 - Train Loss: 0.003403\nBatch 200/209 - Train Loss: 0.003344\nBatch 209/209 - Train Loss: 0.003947\nEpoch 17 Completed ✅ | Training Loss: 0.0031, Validation Loss: 0.0067\n\nEpoch 18/40 --------------------------\nBatch 10/209 - Train Loss: 0.003244\nBatch 20/209 - Train Loss: 0.002236\nBatch 30/209 - Train Loss: 0.002273\nBatch 40/209 - Train Loss: 0.002887\nBatch 50/209 - Train Loss: 0.003156\nBatch 60/209 - Train Loss: 0.002444\nBatch 70/209 - Train Loss: 0.002954\nBatch 80/209 - Train Loss: 0.002483\nBatch 90/209 - Train Loss: 0.003557\nBatch 100/209 - Train Loss: 0.003280\nBatch 110/209 - Train Loss: 0.003849\nBatch 120/209 - Train Loss: 0.002608\nBatch 130/209 - Train Loss: 0.004133\nBatch 140/209 - Train Loss: 0.002792\nBatch 150/209 - Train Loss: 0.001979\nBatch 160/209 - Train Loss: 0.003268\nBatch 170/209 - Train Loss: 0.003035\nBatch 180/209 - Train Loss: 0.003286\nBatch 190/209 - Train Loss: 0.003412\nBatch 200/209 - Train Loss: 0.003102\nBatch 209/209 - Train Loss: 0.004409\nEpoch 18 Completed ✅ | Training Loss: 0.0030, Validation Loss: 0.0064\n\nEpoch 19/40 --------------------------\nBatch 10/209 - Train Loss: 0.002611\nBatch 20/209 - Train Loss: 0.003864\nBatch 30/209 - Train Loss: 0.002608\nBatch 40/209 - Train Loss: 0.003273\nBatch 50/209 - Train Loss: 0.003118\nBatch 60/209 - Train Loss: 0.003457\nBatch 70/209 - Train Loss: 0.002875\nBatch 80/209 - Train Loss: 0.002544\nBatch 90/209 - Train Loss: 0.002882\nBatch 100/209 - Train Loss: 0.003030\nBatch 110/209 - Train Loss: 0.004305\nBatch 120/209 - Train Loss: 0.002386\nBatch 130/209 - Train Loss: 0.002626\nBatch 140/209 - Train Loss: 0.002786\nBatch 150/209 - Train Loss: 0.002513\nBatch 160/209 - Train Loss: 0.002824\nBatch 170/209 - Train Loss: 0.002524\nBatch 180/209 - Train Loss: 0.003262\nBatch 190/209 - Train Loss: 0.002819\nBatch 200/209 - Train Loss: 0.002594\nBatch 209/209 - Train Loss: 0.003099\nEpoch 19 Completed ✅ | Training Loss: 0.0030, Validation Loss: 0.0065\n\nEpoch 20/40 --------------------------\nBatch 10/209 - Train Loss: 0.003080\nBatch 20/209 - Train Loss: 0.002610\nBatch 30/209 - Train Loss: 0.002285\nBatch 40/209 - Train Loss: 0.003188\nBatch 50/209 - Train Loss: 0.002038\nBatch 60/209 - Train Loss: 0.002398\nBatch 70/209 - Train Loss: 0.002694\nBatch 80/209 - Train Loss: 0.003253\nBatch 90/209 - Train Loss: 0.002492\nBatch 100/209 - Train Loss: 0.003080\nBatch 110/209 - Train Loss: 0.003139\nBatch 120/209 - Train Loss: 0.003693\nBatch 130/209 - Train Loss: 0.002948\nBatch 140/209 - Train Loss: 0.003367\nBatch 150/209 - Train Loss: 0.002386\nBatch 160/209 - Train Loss: 0.002258\nBatch 170/209 - Train Loss: 0.002679\nBatch 180/209 - Train Loss: 0.002284\nBatch 190/209 - Train Loss: 0.002119\nBatch 200/209 - Train Loss: 0.002880\nBatch 209/209 - Train Loss: 0.002174\nEpoch 20 Completed ✅ | Training Loss: 0.0028, Validation Loss: 0.0065\n\nEpoch 21/40 --------------------------\nBatch 10/209 - Train Loss: 0.002438\nBatch 20/209 - Train Loss: 0.003098\nBatch 30/209 - Train Loss: 0.003358\nBatch 40/209 - Train Loss: 0.003885\nBatch 50/209 - Train Loss: 0.002001\nBatch 60/209 - Train Loss: 0.002567\nBatch 70/209 - Train Loss: 0.002237\nBatch 80/209 - Train Loss: 0.002828\nBatch 90/209 - Train Loss: 0.003016\nBatch 100/209 - Train Loss: 0.002357\nBatch 110/209 - Train Loss: 0.003563\nBatch 120/209 - Train Loss: 0.002306\nBatch 130/209 - Train Loss: 0.003291\nBatch 140/209 - Train Loss: 0.002316\nBatch 150/209 - Train Loss: 0.001947\nBatch 160/209 - Train Loss: 0.002413\nBatch 170/209 - Train Loss: 0.002099\nBatch 180/209 - Train Loss: 0.002821\nBatch 190/209 - Train Loss: 0.002216\nBatch 200/209 - Train Loss: 0.002669\nBatch 209/209 - Train Loss: 0.002635\nEpoch 21 Completed ✅ | Training Loss: 0.0027, Validation Loss: 0.0064\n\nEpoch 22/40 --------------------------\nBatch 10/209 - Train Loss: 0.002405\nBatch 20/209 - Train Loss: 0.002307\nBatch 30/209 - Train Loss: 0.002979\nBatch 40/209 - Train Loss: 0.002589\nBatch 50/209 - Train Loss: 0.002947\nBatch 60/209 - Train Loss: 0.003064\nBatch 70/209 - Train Loss: 0.002496\nBatch 80/209 - Train Loss: 0.002931\nBatch 90/209 - Train Loss: 0.002603\nBatch 100/209 - Train Loss: 0.002752\nBatch 110/209 - Train Loss: 0.002958\nBatch 120/209 - Train Loss: 0.002562\nBatch 130/209 - Train Loss: 0.002614\nBatch 140/209 - Train Loss: 0.003273\nBatch 150/209 - Train Loss: 0.002416\nBatch 160/209 - Train Loss: 0.002213\nBatch 170/209 - Train Loss: 0.002049\nBatch 180/209 - Train Loss: 0.002624\nBatch 190/209 - Train Loss: 0.002059\nBatch 200/209 - Train Loss: 0.002138\nBatch 209/209 - Train Loss: 0.003043\nEpoch 22 Completed ✅ | Training Loss: 0.0027, Validation Loss: 0.0062\n\nEpoch 23/40 --------------------------\nBatch 10/209 - Train Loss: 0.002370\nBatch 20/209 - Train Loss: 0.002677\nBatch 30/209 - Train Loss: 0.002461\nBatch 40/209 - Train Loss: 0.002709\nBatch 50/209 - Train Loss: 0.002342\nBatch 60/209 - Train Loss: 0.002337\nBatch 70/209 - Train Loss: 0.002538\nBatch 80/209 - Train Loss: 0.002552\nBatch 90/209 - Train Loss: 0.002988\nBatch 100/209 - Train Loss: 0.002105\nBatch 110/209 - Train Loss: 0.001898\nBatch 120/209 - Train Loss: 0.002276\nBatch 130/209 - Train Loss: 0.002341\nBatch 140/209 - Train Loss: 0.002607\nBatch 150/209 - Train Loss: 0.002054\nBatch 160/209 - Train Loss: 0.001842\nBatch 170/209 - Train Loss: 0.002359\nBatch 180/209 - Train Loss: 0.001851\nBatch 190/209 - Train Loss: 0.002727\nBatch 200/209 - Train Loss: 0.002484\nBatch 209/209 - Train Loss: 0.002092\nEpoch 23 Completed ✅ | Training Loss: 0.0025, Validation Loss: 0.0064\n\nEpoch 24/40 --------------------------\nBatch 10/209 - Train Loss: 0.002301\nBatch 20/209 - Train Loss: 0.002416\nBatch 30/209 - Train Loss: 0.002339\nBatch 40/209 - Train Loss: 0.002400\nBatch 50/209 - Train Loss: 0.002362\nBatch 60/209 - Train Loss: 0.001957\nBatch 70/209 - Train Loss: 0.001768\nBatch 80/209 - Train Loss: 0.002494\nBatch 90/209 - Train Loss: 0.002738\nBatch 100/209 - Train Loss: 0.002275\nBatch 110/209 - Train Loss: 0.001825\nBatch 120/209 - Train Loss: 0.002409\nBatch 130/209 - Train Loss: 0.002125\nBatch 140/209 - Train Loss: 0.002424\nBatch 150/209 - Train Loss: 0.002788\nBatch 160/209 - Train Loss: 0.003197\nBatch 170/209 - Train Loss: 0.002699\nBatch 180/209 - Train Loss: 0.001941\nBatch 190/209 - Train Loss: 0.002201\nBatch 200/209 - Train Loss: 0.002682\nBatch 209/209 - Train Loss: 0.002443\nEpoch 24 Completed ✅ | Training Loss: 0.0024, Validation Loss: 0.0062\n\nEpoch 25/40 --------------------------\nBatch 10/209 - Train Loss: 0.002297\nBatch 20/209 - Train Loss: 0.002315\nBatch 30/209 - Train Loss: 0.002445\nBatch 40/209 - Train Loss: 0.002591\nBatch 50/209 - Train Loss: 0.002094\nBatch 60/209 - Train Loss: 0.002447\nBatch 70/209 - Train Loss: 0.002756\nBatch 80/209 - Train Loss: 0.002921\nBatch 90/209 - Train Loss: 0.002527\nBatch 100/209 - Train Loss: 0.002435\nBatch 110/209 - Train Loss: 0.002118\nBatch 120/209 - Train Loss: 0.003862\nBatch 130/209 - Train Loss: 0.002752\nBatch 140/209 - Train Loss: 0.002175\nBatch 150/209 - Train Loss: 0.002249\nBatch 160/209 - Train Loss: 0.001793\nBatch 170/209 - Train Loss: 0.002765\nBatch 180/209 - Train Loss: 0.002253\nBatch 190/209 - Train Loss: 0.002594\nBatch 200/209 - Train Loss: 0.001473\nBatch 209/209 - Train Loss: 0.002392\nEpoch 25 Completed ✅ | Training Loss: 0.0024, Validation Loss: 0.0060\n\nEpoch 26/40 --------------------------\nBatch 10/209 - Train Loss: 0.001798\nBatch 20/209 - Train Loss: 0.002044\nBatch 30/209 - Train Loss: 0.002519\nBatch 40/209 - Train Loss: 0.001977\nBatch 50/209 - Train Loss: 0.002362\nBatch 60/209 - Train Loss: 0.001650\nBatch 70/209 - Train Loss: 0.002057\nBatch 80/209 - Train Loss: 0.002145\nBatch 90/209 - Train Loss: 0.002467\nBatch 100/209 - Train Loss: 0.002270\nBatch 110/209 - Train Loss: 0.002618\nBatch 120/209 - Train Loss: 0.002078\nBatch 130/209 - Train Loss: 0.002234\nBatch 140/209 - Train Loss: 0.003067\nBatch 150/209 - Train Loss: 0.002952\nBatch 160/209 - Train Loss: 0.002637\nBatch 170/209 - Train Loss: 0.002078\nBatch 180/209 - Train Loss: 0.002210\nBatch 190/209 - Train Loss: 0.002111\nBatch 200/209 - Train Loss: 0.002215\nBatch 209/209 - Train Loss: 0.002197\nEpoch 26 Completed ✅ | Training Loss: 0.0023, Validation Loss: 0.0059\n\nEpoch 27/40 --------------------------\nBatch 10/209 - Train Loss: 0.002149\nBatch 20/209 - Train Loss: 0.002599\nBatch 30/209 - Train Loss: 0.002253\nBatch 40/209 - Train Loss: 0.002864\nBatch 50/209 - Train Loss: 0.001973\nBatch 60/209 - Train Loss: 0.002082\nBatch 70/209 - Train Loss: 0.002056\nBatch 80/209 - Train Loss: 0.002111\nBatch 90/209 - Train Loss: 0.002381\nBatch 100/209 - Train Loss: 0.002593\nBatch 110/209 - Train Loss: 0.002364\nBatch 120/209 - Train Loss: 0.001947\nBatch 130/209 - Train Loss: 0.002713\nBatch 140/209 - Train Loss: 0.002802\nBatch 150/209 - Train Loss: 0.002184\nBatch 160/209 - Train Loss: 0.002106\nBatch 170/209 - Train Loss: 0.002174\nBatch 180/209 - Train Loss: 0.002597\nBatch 190/209 - Train Loss: 0.003204\nBatch 200/209 - Train Loss: 0.002739\nBatch 209/209 - Train Loss: 0.001910\nEpoch 27 Completed ✅ | Training Loss: 0.0024, Validation Loss: 0.0063\n\nEpoch 28/40 --------------------------\nBatch 10/209 - Train Loss: 0.002406\nBatch 20/209 - Train Loss: 0.002653\nBatch 30/209 - Train Loss: 0.001974\nBatch 40/209 - Train Loss: 0.001949\nBatch 50/209 - Train Loss: 0.001874\nBatch 60/209 - Train Loss: 0.002288\nBatch 70/209 - Train Loss: 0.004244\nBatch 80/209 - Train Loss: 0.005735\nBatch 90/209 - Train Loss: 0.003586\nBatch 100/209 - Train Loss: 0.003917\nBatch 110/209 - Train Loss: 0.003137\nBatch 120/209 - Train Loss: 0.005429\nBatch 130/209 - Train Loss: 0.003329\nBatch 140/209 - Train Loss: 0.003736\nBatch 150/209 - Train Loss: 0.003402\nBatch 160/209 - Train Loss: 0.004665\nBatch 170/209 - Train Loss: 0.003499\nBatch 180/209 - Train Loss: 0.003824\nBatch 190/209 - Train Loss: 0.003513\nBatch 200/209 - Train Loss: 0.003303\nBatch 209/209 - Train Loss: 0.003763\nEpoch 28 Completed ✅ | Training Loss: 0.0032, Validation Loss: 0.0075\n\nEpoch 29/40 --------------------------\nBatch 10/209 - Train Loss: 0.003600\nBatch 20/209 - Train Loss: 0.007701\nBatch 30/209 - Train Loss: 0.004215\nBatch 40/209 - Train Loss: 0.004562\nBatch 50/209 - Train Loss: 0.003473\nBatch 60/209 - Train Loss: 0.004080\nBatch 70/209 - Train Loss: 0.003357\nBatch 80/209 - Train Loss: 0.004564\nBatch 90/209 - Train Loss: 0.003963\nBatch 100/209 - Train Loss: 0.003220\nBatch 110/209 - Train Loss: 0.004017\nBatch 120/209 - Train Loss: 0.003173\nBatch 130/209 - Train Loss: 0.002719\nBatch 140/209 - Train Loss: 0.004101\nBatch 150/209 - Train Loss: 0.003176\nBatch 160/209 - Train Loss: 0.002993\nBatch 170/209 - Train Loss: 0.003147\nBatch 180/209 - Train Loss: 0.003677\nBatch 190/209 - Train Loss: 0.003650\nBatch 200/209 - Train Loss: 0.002632\nBatch 209/209 - Train Loss: 0.003803\nEpoch 29 Completed ✅ | Training Loss: 0.0036, Validation Loss: 0.0079\n\nEpoch 30/40 --------------------------\nBatch 10/209 - Train Loss: 0.003302\nBatch 20/209 - Train Loss: 0.002896\nBatch 30/209 - Train Loss: 0.003363\nBatch 40/209 - Train Loss: 0.003008\nBatch 50/209 - Train Loss: 0.003634\nBatch 60/209 - Train Loss: 0.002965\nBatch 70/209 - Train Loss: 0.002391\nBatch 80/209 - Train Loss: 0.003485\nBatch 90/209 - Train Loss: 0.003509\nBatch 100/209 - Train Loss: 0.002891\nBatch 110/209 - Train Loss: 0.002792\nBatch 120/209 - Train Loss: 0.003554\nBatch 130/209 - Train Loss: 0.002566\nBatch 140/209 - Train Loss: 0.002528\nBatch 150/209 - Train Loss: 0.002884\nBatch 160/209 - Train Loss: 0.002534\nBatch 170/209 - Train Loss: 0.002706\nBatch 180/209 - Train Loss: 0.002455\nBatch 190/209 - Train Loss: 0.002441\nBatch 200/209 - Train Loss: 0.003569\nBatch 209/209 - Train Loss: 0.003693\nEpoch 30 Completed ✅ | Training Loss: 0.0032, Validation Loss: 0.0062\n\nEpoch 31/40 --------------------------\nBatch 10/209 - Train Loss: 0.003055\nBatch 20/209 - Train Loss: 0.002197\nBatch 30/209 - Train Loss: 0.002688\nBatch 40/209 - Train Loss: 0.002663\nBatch 50/209 - Train Loss: 0.003019\nBatch 60/209 - Train Loss: 0.002631\nBatch 70/209 - Train Loss: 0.002811\nBatch 80/209 - Train Loss: 0.002752\nBatch 90/209 - Train Loss: 0.002663\nBatch 100/209 - Train Loss: 0.002605\nBatch 110/209 - Train Loss: 0.002166\nBatch 120/209 - Train Loss: 0.002082\nBatch 130/209 - Train Loss: 0.002441\nBatch 140/209 - Train Loss: 0.002135\nBatch 150/209 - Train Loss: 0.002685\nBatch 160/209 - Train Loss: 0.001753\nBatch 170/209 - Train Loss: 0.001874\nBatch 180/209 - Train Loss: 0.002585\nBatch 190/209 - Train Loss: 0.002373\nBatch 200/209 - Train Loss: 0.002021\nBatch 209/209 - Train Loss: 0.002693\nEpoch 31 Completed ✅ | Training Loss: 0.0025, Validation Loss: 0.0058\n\nEpoch 32/40 --------------------------\nBatch 10/209 - Train Loss: 0.001801\nBatch 20/209 - Train Loss: 0.002142\nBatch 30/209 - Train Loss: 0.002569\nBatch 40/209 - Train Loss: 0.002375\nBatch 50/209 - Train Loss: 0.001846\nBatch 60/209 - Train Loss: 0.001808\nBatch 70/209 - Train Loss: 0.002221\nBatch 80/209 - Train Loss: 0.001984\nBatch 90/209 - Train Loss: 0.001936\nBatch 100/209 - Train Loss: 0.002547\nBatch 110/209 - Train Loss: 0.002620\nBatch 120/209 - Train Loss: 0.002714\nBatch 130/209 - Train Loss: 0.002264\nBatch 140/209 - Train Loss: 0.001909\nBatch 150/209 - Train Loss: 0.002159\nBatch 160/209 - Train Loss: 0.002000\nBatch 170/209 - Train Loss: 0.002382\nBatch 180/209 - Train Loss: 0.002734\nBatch 190/209 - Train Loss: 0.002320\nBatch 200/209 - Train Loss: 0.002121\nBatch 209/209 - Train Loss: 0.001785\nEpoch 32 Completed ✅ | Training Loss: 0.0022, Validation Loss: 0.0056\n\nEpoch 33/40 --------------------------\nBatch 10/209 - Train Loss: 0.001636\nBatch 20/209 - Train Loss: 0.001706\nBatch 30/209 - Train Loss: 0.001860\nBatch 40/209 - Train Loss: 0.002055\nBatch 50/209 - Train Loss: 0.002338\nBatch 60/209 - Train Loss: 0.001739\nBatch 70/209 - Train Loss: 0.001883\nBatch 80/209 - Train Loss: 0.001525\nBatch 90/209 - Train Loss: 0.002228\nBatch 100/209 - Train Loss: 0.001858\nBatch 110/209 - Train Loss: 0.001727\nBatch 120/209 - Train Loss: 0.002201\nBatch 130/209 - Train Loss: 0.001508\nBatch 140/209 - Train Loss: 0.002011\nBatch 150/209 - Train Loss: 0.001950\nBatch 160/209 - Train Loss: 0.001958\nBatch 170/209 - Train Loss: 0.002394\nBatch 180/209 - Train Loss: 0.002158\nBatch 190/209 - Train Loss: 0.001883\nBatch 200/209 - Train Loss: 0.001887\nBatch 209/209 - Train Loss: 0.001832\nEpoch 33 Completed ✅ | Training Loss: 0.0020, Validation Loss: 0.0054\n\nEpoch 34/40 --------------------------\nBatch 10/209 - Train Loss: 0.001662\nBatch 20/209 - Train Loss: 0.002024\nBatch 30/209 - Train Loss: 0.001671\nBatch 40/209 - Train Loss: 0.001398\nBatch 50/209 - Train Loss: 0.001938\nBatch 60/209 - Train Loss: 0.001795\nBatch 70/209 - Train Loss: 0.001860\nBatch 80/209 - Train Loss: 0.001638\nBatch 90/209 - Train Loss: 0.002373\nBatch 100/209 - Train Loss: 0.001744\nBatch 110/209 - Train Loss: 0.001592\nBatch 120/209 - Train Loss: 0.002491\nBatch 130/209 - Train Loss: 0.001597\nBatch 140/209 - Train Loss: 0.001980\nBatch 150/209 - Train Loss: 0.001867\nBatch 160/209 - Train Loss: 0.001773\nBatch 170/209 - Train Loss: 0.002070\nBatch 180/209 - Train Loss: 0.001603\nBatch 190/209 - Train Loss: 0.002235\nBatch 200/209 - Train Loss: 0.001816\nBatch 209/209 - Train Loss: 0.001361\nEpoch 34 Completed ✅ | Training Loss: 0.0018, Validation Loss: 0.0054\n\nEpoch 35/40 --------------------------\nBatch 10/209 - Train Loss: 0.001452\nBatch 20/209 - Train Loss: 0.002216\nBatch 30/209 - Train Loss: 0.001630\nBatch 40/209 - Train Loss: 0.001825\nBatch 50/209 - Train Loss: 0.001705\nBatch 60/209 - Train Loss: 0.001499\nBatch 70/209 - Train Loss: 0.001974\nBatch 80/209 - Train Loss: 0.001489\nBatch 90/209 - Train Loss: 0.001727\nBatch 100/209 - Train Loss: 0.002055\nBatch 110/209 - Train Loss: 0.001759\nBatch 120/209 - Train Loss: 0.001670\nBatch 130/209 - Train Loss: 0.001721\nBatch 140/209 - Train Loss: 0.001425\nBatch 150/209 - Train Loss: 0.001880\nBatch 160/209 - Train Loss: 0.002025\nBatch 170/209 - Train Loss: 0.001890\nBatch 180/209 - Train Loss: 0.001722\nBatch 190/209 - Train Loss: 0.001987\nBatch 200/209 - Train Loss: 0.001672\nBatch 209/209 - Train Loss: 0.001808\nEpoch 35 Completed ✅ | Training Loss: 0.0018, Validation Loss: 0.0054\n\nEpoch 36/40 --------------------------\nBatch 10/209 - Train Loss: 0.002354\nBatch 20/209 - Train Loss: 0.001282\nBatch 30/209 - Train Loss: 0.001686\nBatch 40/209 - Train Loss: 0.001472\nBatch 50/209 - Train Loss: 0.001740\nBatch 60/209 - Train Loss: 0.002319\nBatch 70/209 - Train Loss: 0.001822\nBatch 80/209 - Train Loss: 0.002508\nBatch 90/209 - Train Loss: 0.001623\nBatch 100/209 - Train Loss: 0.001226\nBatch 110/209 - Train Loss: 0.002002\nBatch 120/209 - Train Loss: 0.001397\nBatch 130/209 - Train Loss: 0.001709\nBatch 140/209 - Train Loss: 0.001931\nBatch 150/209 - Train Loss: 0.002014\nBatch 160/209 - Train Loss: 0.001526\nBatch 170/209 - Train Loss: 0.001842\nBatch 180/209 - Train Loss: 0.001500\nBatch 190/209 - Train Loss: 0.001273\nBatch 200/209 - Train Loss: 0.001780\nBatch 209/209 - Train Loss: 0.001816\nEpoch 36 Completed ✅ | Training Loss: 0.0017, Validation Loss: 0.0054\n\nEpoch 37/40 --------------------------\nBatch 10/209 - Train Loss: 0.001672\nBatch 20/209 - Train Loss: 0.001848\nBatch 30/209 - Train Loss: 0.001508\nBatch 40/209 - Train Loss: 0.002501\nBatch 50/209 - Train Loss: 0.001817\nBatch 60/209 - Train Loss: 0.001465\nBatch 70/209 - Train Loss: 0.001827\nBatch 80/209 - Train Loss: 0.001720\nBatch 90/209 - Train Loss: 0.001678\nBatch 100/209 - Train Loss: 0.001823\nBatch 110/209 - Train Loss: 0.001481\nBatch 120/209 - Train Loss: 0.001434\nBatch 130/209 - Train Loss: 0.001205\nBatch 140/209 - Train Loss: 0.001508\nBatch 150/209 - Train Loss: 0.002339\nBatch 160/209 - Train Loss: 0.001396\nBatch 170/209 - Train Loss: 0.001741\nBatch 180/209 - Train Loss: 0.001804\nBatch 190/209 - Train Loss: 0.001448\nBatch 200/209 - Train Loss: 0.002529\nBatch 209/209 - Train Loss: 0.001578\nEpoch 37 Completed ✅ | Training Loss: 0.0016, Validation Loss: 0.0053\n\nEpoch 38/40 --------------------------\nBatch 10/209 - Train Loss: 0.001854\nBatch 20/209 - Train Loss: 0.001492\nBatch 30/209 - Train Loss: 0.001818\nBatch 40/209 - Train Loss: 0.002194\nBatch 50/209 - Train Loss: 0.001199\nBatch 60/209 - Train Loss: 0.001555\nBatch 70/209 - Train Loss: 0.001655\nBatch 80/209 - Train Loss: 0.001617\nBatch 90/209 - Train Loss: 0.001795\nBatch 100/209 - Train Loss: 0.001723\nBatch 110/209 - Train Loss: 0.001417\nBatch 120/209 - Train Loss: 0.001391\nBatch 130/209 - Train Loss: 0.001894\nBatch 140/209 - Train Loss: 0.001630\nBatch 150/209 - Train Loss: 0.001653\nBatch 160/209 - Train Loss: 0.001813\nBatch 170/209 - Train Loss: 0.001229\nBatch 180/209 - Train Loss: 0.001400\nBatch 190/209 - Train Loss: 0.001510\nBatch 200/209 - Train Loss: 0.001688\nBatch 209/209 - Train Loss: 0.001329\nEpoch 38 Completed ✅ | Training Loss: 0.0016, Validation Loss: 0.0054\n\nEpoch 39/40 --------------------------\nBatch 10/209 - Train Loss: 0.001585\nBatch 20/209 - Train Loss: 0.001459\nBatch 30/209 - Train Loss: 0.001599\nBatch 40/209 - Train Loss: 0.002176\nBatch 50/209 - Train Loss: 0.001817\nBatch 60/209 - Train Loss: 0.001120\nBatch 70/209 - Train Loss: 0.001864\nBatch 80/209 - Train Loss: 0.002041\nBatch 90/209 - Train Loss: 0.001641\nBatch 100/209 - Train Loss: 0.001712\nBatch 110/209 - Train Loss: 0.001443\nBatch 120/209 - Train Loss: 0.001670\nBatch 130/209 - Train Loss: 0.001618\nBatch 140/209 - Train Loss: 0.001509\nBatch 150/209 - Train Loss: 0.001381\nBatch 160/209 - Train Loss: 0.001682\nBatch 170/209 - Train Loss: 0.001818\nBatch 180/209 - Train Loss: 0.001525\nBatch 190/209 - Train Loss: 0.001583\nBatch 200/209 - Train Loss: 0.001519\nBatch 209/209 - Train Loss: 0.001457\nEpoch 39 Completed ✅ | Training Loss: 0.0016, Validation Loss: 0.0054\n\nEpoch 40/40 --------------------------\nBatch 10/209 - Train Loss: 0.001412\nBatch 20/209 - Train Loss: 0.001740\nBatch 30/209 - Train Loss: 0.001817\nBatch 40/209 - Train Loss: 0.001451\nBatch 50/209 - Train Loss: 0.001568\nBatch 60/209 - Train Loss: 0.001162\nBatch 70/209 - Train Loss: 0.001512\nBatch 80/209 - Train Loss: 0.001612\nBatch 90/209 - Train Loss: 0.001700\nBatch 100/209 - Train Loss: 0.001424\nBatch 110/209 - Train Loss: 0.001830\nBatch 120/209 - Train Loss: 0.001790\nBatch 130/209 - Train Loss: 0.001577\nBatch 140/209 - Train Loss: 0.001559\nBatch 150/209 - Train Loss: 0.001584\nBatch 160/209 - Train Loss: 0.001483\nBatch 170/209 - Train Loss: 0.002151\nBatch 180/209 - Train Loss: 0.001504\nBatch 190/209 - Train Loss: 0.001769\nBatch 200/209 - Train Loss: 0.001073\nBatch 209/209 - Train Loss: 0.001166\nEpoch 40 Completed ✅ | Training Loss: 0.0016, Validation Loss: 0.0053\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Save model\ntorch.save(model.state_dict(), 'depth_estimation_model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T10:44:51.294709Z","iopub.execute_input":"2025-03-29T10:44:51.294919Z","iopub.status.idle":"2025-03-29T10:44:51.633434Z","shell.execute_reply.started":"2025-03-29T10:44:51.294893Z","shell.execute_reply":"2025-03-29T10:44:51.632727Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Generate and save predictions\npredictions, image_names = generate_predictions(model, test_loader)\n\n# Convert predictions to CSV\nimages_to_csv_with_metadata(predictions, image_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T10:44:51.634274Z","iopub.execute_input":"2025-03-29T10:44:51.634519Z","iopub.status.idle":"2025-03-29T10:45:17.321926Z","shell.execute_reply.started":"2025-03-29T10:44:51.634499Z","shell.execute_reply":"2025-03-29T10:45:17.321032Z"}},"outputs":[{"name":"stdout","text":"Predictions saved to predictions.csv\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"df = pd.read_csv(\"predictions.csv\")\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T10:45:17.324669Z","iopub.execute_input":"2025-03-29T10:45:17.324910Z","iopub.status.idle":"2025-03-29T10:45:20.732942Z","shell.execute_reply.started":"2025-03-29T10:45:17.324883Z","shell.execute_reply":"2025-03-29T10:45:20.732127Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"      id       ImageID   0   1   2   3   4   5   6   7  ...  16374  16375  \\\n0      0  10052011.png  42  41  42  41  41  42  42  41  ...    246    246   \n1      1  10052012.png  13  11  12  12  12  12  12  12  ...    248    248   \n2      2  10152031.png  51  51  50  48  47  45  44  44  ...    246    247   \n3      3  10152032.png  18  16  16  16  16  16  16  16  ...    250    251   \n4      4  10252051.png  56  57  58  56  55  54  53  52  ...    254    254   \n..   ...           ...  ..  ..  ..  ..  ..  ..  ..  ..  ...    ...    ...   \n831  831   9751952.png  13  10  11  11  11  11  11  11  ...    251    252   \n832  832   9851971.png  47  46  47  46  46  46  46  46  ...    229    230   \n833  833   9851972.png  18  15  16  16  16  16  16  16  ...    251    252   \n834  834   9951991.png  47  46  47  48  48  48  48  48  ...    252    252   \n835  835   9951992.png  22  20  20  19  19  19  19  19  ...    248    249   \n\n     16376  16377  16378  16379  16380  16381  16382  16383  \n0      246    246    246    247    248    248    248    243  \n1      249    249    249    249    251    254    253    245  \n2      247    248    249    250    252    254    254    252  \n3      252    252    252    252    253    254    254    246  \n4      254    254    254    253    254    254    253    246  \n..     ...    ...    ...    ...    ...    ...    ...    ...  \n831    252    253    253    253    254    254    254    246  \n832    231    232    233    234    236    239    242    239  \n833    252    252    252    253    254    254    254    247  \n834    252    252    251    251    252    252    251    247  \n835    249    250    250    251    253    254    253    246  \n\n[836 rows x 16386 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>ImageID</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>...</th>\n      <th>16374</th>\n      <th>16375</th>\n      <th>16376</th>\n      <th>16377</th>\n      <th>16378</th>\n      <th>16379</th>\n      <th>16380</th>\n      <th>16381</th>\n      <th>16382</th>\n      <th>16383</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>10052011.png</td>\n      <td>42</td>\n      <td>41</td>\n      <td>42</td>\n      <td>41</td>\n      <td>41</td>\n      <td>42</td>\n      <td>42</td>\n      <td>41</td>\n      <td>...</td>\n      <td>246</td>\n      <td>246</td>\n      <td>246</td>\n      <td>246</td>\n      <td>246</td>\n      <td>247</td>\n      <td>248</td>\n      <td>248</td>\n      <td>248</td>\n      <td>243</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>10052012.png</td>\n      <td>13</td>\n      <td>11</td>\n      <td>12</td>\n      <td>12</td>\n      <td>12</td>\n      <td>12</td>\n      <td>12</td>\n      <td>12</td>\n      <td>...</td>\n      <td>248</td>\n      <td>248</td>\n      <td>249</td>\n      <td>249</td>\n      <td>249</td>\n      <td>249</td>\n      <td>251</td>\n      <td>254</td>\n      <td>253</td>\n      <td>245</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>10152031.png</td>\n      <td>51</td>\n      <td>51</td>\n      <td>50</td>\n      <td>48</td>\n      <td>47</td>\n      <td>45</td>\n      <td>44</td>\n      <td>44</td>\n      <td>...</td>\n      <td>246</td>\n      <td>247</td>\n      <td>247</td>\n      <td>248</td>\n      <td>249</td>\n      <td>250</td>\n      <td>252</td>\n      <td>254</td>\n      <td>254</td>\n      <td>252</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>10152032.png</td>\n      <td>18</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>...</td>\n      <td>250</td>\n      <td>251</td>\n      <td>252</td>\n      <td>252</td>\n      <td>252</td>\n      <td>252</td>\n      <td>253</td>\n      <td>254</td>\n      <td>254</td>\n      <td>246</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>10252051.png</td>\n      <td>56</td>\n      <td>57</td>\n      <td>58</td>\n      <td>56</td>\n      <td>55</td>\n      <td>54</td>\n      <td>53</td>\n      <td>52</td>\n      <td>...</td>\n      <td>254</td>\n      <td>254</td>\n      <td>254</td>\n      <td>254</td>\n      <td>254</td>\n      <td>253</td>\n      <td>254</td>\n      <td>254</td>\n      <td>253</td>\n      <td>246</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>831</th>\n      <td>831</td>\n      <td>9751952.png</td>\n      <td>13</td>\n      <td>10</td>\n      <td>11</td>\n      <td>11</td>\n      <td>11</td>\n      <td>11</td>\n      <td>11</td>\n      <td>11</td>\n      <td>...</td>\n      <td>251</td>\n      <td>252</td>\n      <td>252</td>\n      <td>253</td>\n      <td>253</td>\n      <td>253</td>\n      <td>254</td>\n      <td>254</td>\n      <td>254</td>\n      <td>246</td>\n    </tr>\n    <tr>\n      <th>832</th>\n      <td>832</td>\n      <td>9851971.png</td>\n      <td>47</td>\n      <td>46</td>\n      <td>47</td>\n      <td>46</td>\n      <td>46</td>\n      <td>46</td>\n      <td>46</td>\n      <td>46</td>\n      <td>...</td>\n      <td>229</td>\n      <td>230</td>\n      <td>231</td>\n      <td>232</td>\n      <td>233</td>\n      <td>234</td>\n      <td>236</td>\n      <td>239</td>\n      <td>242</td>\n      <td>239</td>\n    </tr>\n    <tr>\n      <th>833</th>\n      <td>833</td>\n      <td>9851972.png</td>\n      <td>18</td>\n      <td>15</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>16</td>\n      <td>...</td>\n      <td>251</td>\n      <td>252</td>\n      <td>252</td>\n      <td>252</td>\n      <td>252</td>\n      <td>253</td>\n      <td>254</td>\n      <td>254</td>\n      <td>254</td>\n      <td>247</td>\n    </tr>\n    <tr>\n      <th>834</th>\n      <td>834</td>\n      <td>9951991.png</td>\n      <td>47</td>\n      <td>46</td>\n      <td>47</td>\n      <td>48</td>\n      <td>48</td>\n      <td>48</td>\n      <td>48</td>\n      <td>48</td>\n      <td>...</td>\n      <td>252</td>\n      <td>252</td>\n      <td>252</td>\n      <td>252</td>\n      <td>251</td>\n      <td>251</td>\n      <td>252</td>\n      <td>252</td>\n      <td>251</td>\n      <td>247</td>\n    </tr>\n    <tr>\n      <th>835</th>\n      <td>835</td>\n      <td>9951992.png</td>\n      <td>22</td>\n      <td>20</td>\n      <td>20</td>\n      <td>19</td>\n      <td>19</td>\n      <td>19</td>\n      <td>19</td>\n      <td>19</td>\n      <td>...</td>\n      <td>248</td>\n      <td>249</td>\n      <td>249</td>\n      <td>250</td>\n      <td>250</td>\n      <td>251</td>\n      <td>253</td>\n      <td>254</td>\n      <td>253</td>\n      <td>246</td>\n    </tr>\n  </tbody>\n</table>\n<p>836 rows × 16386 columns</p>\n</div>"},"metadata":{}}],"execution_count":13}]}