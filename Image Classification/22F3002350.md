# Vision Transformer (ViT) Image Classification

This project implements a fine-tuned Vision Transformer (ViT) model for 10-class image classification.

## Overview

The implementation uses a pre-trained Vision Transformer (ViT-B/16) model which is fine-tuned on a custom dataset. The model achieves high accuracy through transfer learning and custom modifications to the classifier head.

## Requirements

- Python 3.6+
- PyTorch 1.7+
- torchvision
- scikit-learn
- pandas
- PIL
- CUDA-capable GPU (recommended)

## Installation

```bash
pip install torch torchvision sklearn pandas Pillow
```

## Dataset Structure

The dataset should be organized as follows:

```
/dataset
  /train
    /class1
      img1.jpg
      img2.jpg
      ...
    /class2
      img1.jpg
      ...
    ...
  /test
    img1.jpg
    img2.jpg
    ...
```

## Running the Code

1. Clone this repository or download the Jupyter notebook
2. Update the data paths in the notebook to point to your dataset:
   ```python
   train_data_dir = '/path/to/your/train/folder'
   test_data_dir = '/path/to/your/test/folder'
   ```
3. Run the notebook cells sequentially
4. The submission file will be generated as 'submission.csv'

## Model Architecture

The model architecture is based on the pre-trained Vision Transformer (ViT-B/16) with a customized classifier head:

- Base model: ViT-B/16 (pretrained on ImageNet)
- Custom classifier head:
  - Dropout (0.5)
  - Linear layer (768 → 512)
  - ReLU activation
  - Dropout (0.3)
  - Linear layer (512 → 10 classes)

## Training Strategy

- **Optimizer**: AdamW with weight decay of 1e-4
- **Learning Rate**: 3e-4 with OneCycleLR scheduler
- **Loss Function**: Cross Entropy Loss
- **Batch Size**: 64
- **Epochs**: 10
- **Data Augmentation**:
  - Random resized crop (224×224)
  - Random horizontal flip
  - Normalization using ImageNet statistics

## Why Vision Transformer?

The Vision Transformer (ViT) was selected for this classification task for several compelling reasons:

1. **State-of-the-art performance**: ViT models have demonstrated exceptional performance on image classification tasks, often outperforming CNN-based architectures like ResNet on benchmark datasets.

2. **Self-attention mechanism**: The transformer's ability to model long-range dependencies between image patches allows it to capture global context more effectively than CNNs with their limited receptive fields.

3. **Transfer learning efficiency**: The pre-trained ViT model encodes robust, generalizable features that transfer well to new domains with limited data.

4. **Scalability**: Transformer architectures scale effectively with more data and compute resources.

5. **Less inductive bias**: Unlike CNNs, ViT makes fewer assumptions about the structure of images, potentially allowing it to learn more flexible representations.

The custom classifier head with dropout layers helps prevent overfitting while the additional linear layer provides an intermediate representation space before final classification, improving model generalization.

## Performance Evaluation

The model is evaluated using macro F1-score on the validation set. This metric was chosen because it provides a balanced assessment of performance across all classes, which is especially important if the dataset has class imbalance.

## Results

After training for 10 epochs, the model achieves:
- Training Loss: 0.0391
- Validation Loss: 0.6245
- Macro F1 Score: 0.8567

## Future Improvements

Potential improvements to explore:
- Experiment with different ViT variants (e.g., ViT-L/16)
- Implement test-time augmentation
- Try different learning rate schedules
- Experiment with different dropout rates
- Implement ensemble methods with other model architectures
