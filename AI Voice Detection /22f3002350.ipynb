{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":95302,"databundleVersionId":11325230,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:29:06.718900Z","iopub.execute_input":"2025-03-09T06:29:06.719190Z","iopub.status.idle":"2025-03-09T06:29:07.484119Z","shell.execute_reply.started":"2025-03-09T06:29:06.719168Z","shell.execute_reply":"2025-03-09T06:29:07.483252Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/indic-tts-deepfake-challenge/sample.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom transformers import Trainer, TrainingArguments, AutoModelForAudioClassification, Wav2Vec2Processor, DataCollatorWithPadding\nfrom sklearn.metrics import roc_auc_score\nfrom datasets import Dataset, load_dataset\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoFeatureExtractor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:29:07.484935Z","iopub.execute_input":"2025-03-09T06:29:07.485369Z","iopub.status.idle":"2025-03-09T06:29:29.565989Z","shell.execute_reply.started":"2025-03-09T06:29:07.485330Z","shell.execute_reply":"2025-03-09T06:29:29.565116Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load the dataset from Hugging Face\ndataset = load_dataset(\"SherryT997/IndicTTS-Deepfake-Challenge-Data\")\n\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:29:29.566889Z","iopub.execute_input":"2025-03-09T06:29:29.567444Z","iopub.status.idle":"2025-03-09T06:33:43.206716Z","shell.execute_reply.started":"2025-03-09T06:29:29.567411Z","shell.execute_reply":"2025-03-09T06:33:43.206146Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/2.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"389726ebeb26435388d6ea10fbeb9276"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bd6994330b44c019ed241b5a55e917c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08a5e0ac23a342579abb45159bc3f412"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0/35 [00:00<?, ?files/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d79bfc7305b4b909ba71bbf465a1ff4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00035.parquet:   0%|          | 0.00/453M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"225523f434a44251888ac1ec4eacc7b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00035.parquet:   0%|          | 0.00/461M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d8ada1da550435a92de68ecc7994365"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00035.parquet:   0%|          | 0.00/464M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f19fbaf861543d89935337a532f9089"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00003-of-00035.parquet:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b87ba36a69804175834612ad60e1cf3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00004-of-00035.parquet:   0%|          | 0.00/470M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff8a86e451c141e8a12e6c8ead6df04c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00005-of-00035.parquet:   0%|          | 0.00/475M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38df8211ecbd46bf9d5356021c295144"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00006-of-00035.parquet:   0%|          | 0.00/447M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8921569c8f3a4dc4a01dd586ffed25ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00007-of-00035.parquet:   0%|          | 0.00/516M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64b2ae97fd914b18b11b8ed1702efb8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00008-of-00035.parquet:   0%|          | 0.00/557M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b466d4d1a4c4cad852b67a307d5a19c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00009-of-00035.parquet:   0%|          | 0.00/521M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e87295efdf04eb4935d8ca31bbd30bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00010-of-00035.parquet:   0%|          | 0.00/491M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e656ae73b194d3fb7bcdbebd0273d30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00011-of-00035.parquet:   0%|          | 0.00/426M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c77815e9b9b4b99ba5f1422f4f8aee4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00012-of-00035.parquet:   0%|          | 0.00/414M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"617877ca9b154647b973621bcbb0a4b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00013-of-00035.parquet:   0%|          | 0.00/473M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfd92e544f2b42debb4d34b59c14317a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00014-of-00035.parquet:   0%|          | 0.00/481M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d91986e2b26457ea67a597563eda0a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00015-of-00035.parquet:   0%|          | 0.00/467M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99506ccb03044277a96823aa5bd9a122"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00016-of-00035.parquet:   0%|          | 0.00/532M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3a6b65260b247f0b934b449ba47195c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00017-of-00035.parquet:   0%|          | 0.00/510M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04523d270a8240b5bccc8479a9ec4272"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00018-of-00035.parquet:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3937596ed06441c4a9e303253a541638"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00019-of-00035.parquet:   0%|          | 0.00/501M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f29b2c13c814b809a88dae8e8915580"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00020-of-00035.parquet:   0%|          | 0.00/559M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b41354348ed546999ba8d2bd8c87282f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00021-of-00035.parquet:   0%|          | 0.00/541M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13d9aa8ce94c482aaf0c622121003a7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00022-of-00035.parquet:   0%|          | 0.00/558M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85c489e4817049d6aa5d9fe5561ceb3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00023-of-00035.parquet:   0%|          | 0.00/599M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13520b09fa574f55bd450dad0807d177"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00024-of-00035.parquet:   0%|          | 0.00/576M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad9d31251ffa4ae1b929f2d84e5b06ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00025-of-00035.parquet:   0%|          | 0.00/547M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b2b759d301040cdac0855fac3700c5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00026-of-00035.parquet:   0%|          | 0.00/537M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4685d1f6da64001bfeca4caa0249724"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00027-of-00035.parquet:   0%|          | 0.00/421M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fbc33a3f73a4efcae08a05ea193e6d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00028-of-00035.parquet:   0%|          | 0.00/382M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"189a81c4517f4fa08de4e7dbe11c6c5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00029-of-00035.parquet:   0%|          | 0.00/287M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f60da6844564de6b2f2d866b4675217"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00030-of-00035.parquet:   0%|          | 0.00/282M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"964dcada90f94debb479c709d52b5227"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00031-of-00035.parquet:   0%|          | 0.00/688M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d048685e187d4244bc61905bc722257e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00032-of-00035.parquet:   0%|          | 0.00/613M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"332792d8d6034cbea29ee2abb7989b33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00033-of-00035.parquet:   0%|          | 0.00/309M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccdbadd0913a49bf9548e0463b2f6d79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00034-of-00035.parquet:   0%|          | 0.00/424M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb1abbe2a1ec4fb8ad5a32f904446edb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00004.parquet:   0%|          | 0.00/356M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61c520e69790441fb0d7b4521fde231e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00001-of-00004.parquet:   0%|          | 0.00/364M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdc445f7fced4ff2a1e461f16e375b43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00002-of-00004.parquet:   0%|          | 0.00/410M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aed34428a5e4125915b68c8b2e13081"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00003-of-00004.parquet:   0%|          | 0.00/291M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"116966fd6d0a40bfb23900ab66a7ce83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/31102 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e8491396baa4fb1bc7f423d9362f54f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2635 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c62b257833fe43b6903a3e2e3a009a5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading dataset shards:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa713858326a407daf624ca0fb93c8e5"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'id', 'language', 'is_tts', 'audio'],\n        num_rows: 31102\n    })\n    test: Dataset({\n        features: ['text', 'id', 'language', 'is_tts', 'audio'],\n        num_rows: 2635\n    })\n})"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:33:43.207436Z","iopub.execute_input":"2025-03-09T06:33:43.207729Z","iopub.status.idle":"2025-03-09T06:33:43.410976Z","shell.execute_reply.started":"2025-03-09T06:33:43.207697Z","shell.execute_reply":"2025-03-09T06:33:43.410302Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab5f260822d4415e84e521ba00cbac93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.84k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00cd750267854b7e807e9e9708e21c9a"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def preprocess_function(examples):\n    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n    inputs = feature_extractor(\n        audio_arrays, sampling_rate=16000, max_length=16000, truncation=True, padding=True\n    )\n    return inputs\n\npreprocessed = dataset['train'].map(preprocess_function, remove_columns=[\"audio\",\"text\",\"id\",\"language\"], batched=True)\npreprocessed = preprocessed.rename_column(\"is_tts\", \"label\")\n\npreprocessed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:33:43.411619Z","iopub.execute_input":"2025-03-09T06:33:43.411836Z","iopub.status.idle":"2025-03-09T06:38:39.792020Z","shell.execute_reply.started":"2025-03-09T06:33:43.411816Z","shell.execute_reply":"2025-03-09T06:38:39.791321Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/31102 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6169893042d4aeeadcf5b57e9538779"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['label', 'input_values'],\n    num_rows: 31102\n})"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Load Wav2Vec2 Processor and Model\nmodel = AutoModelForAudioClassification.from_pretrained(\"facebook/wav2vec2-base-960h\", num_labels=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:38:39.793932Z","iopub.execute_input":"2025-03-09T06:38:39.794458Z","iopub.status.idle":"2025-03-09T06:38:41.987297Z","shell.execute_reply.started":"2025-03-09T06:38:39.794434Z","shell.execute_reply":"2025-03-09T06:38:41.986647Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a6eb062e75c4211980534b7544fc5af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2367a99ec5994b6c9e6618757f9c30d6"}},"metadata":{}},{"name":"stderr","text":"Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Function to compute ROC-AUC for evaluation\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    # Get probabilities for positive class (index 1)\n    preds = torch.nn.functional.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()\n    auc = roc_auc_score(labels, preds) if len(set(labels)) > 1 else 0.5\n    return {\"roc_auc\": auc}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:38:41.988854Z","iopub.execute_input":"2025-03-09T06:38:41.989188Z","iopub.status.idle":"2025-03-09T06:38:41.993648Z","shell.execute_reply.started":"2025-03-09T06:38:41.989163Z","shell.execute_reply":"2025-03-09T06:38:41.992868Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train_eval_dataset = preprocessed.train_test_split(test_size=0.1)\n\ntrain_eval_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:38:41.994417Z","iopub.execute_input":"2025-03-09T06:38:41.994695Z","iopub.status.idle":"2025-03-09T06:38:42.027934Z","shell.execute_reply.started":"2025-03-09T06:38:41.994667Z","shell.execute_reply":"2025-03-09T06:38:42.027035Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['label', 'input_values'],\n        num_rows: 27991\n    })\n    test: Dataset({\n        features: ['label', 'input_values'],\n        num_rows: 3111\n    })\n})"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"final_model\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=64,\n    gradient_accumulation_steps=4,\n    per_device_eval_batch_size=64,\n    num_train_epochs=10,\n    warmup_ratio=0.05,\n    logging_steps=50,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_roc_auc\",\n    report_to=\"none\"\n)\n    \ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_eval_dataset[\"train\"],\n    eval_dataset=train_eval_dataset[\"test\"],\n    processing_class=feature_extractor,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:38:42.028769Z","iopub.execute_input":"2025-03-09T06:38:42.029004Z","iopub.status.idle":"2025-03-09T06:38:42.577340Z","shell.execute_reply.started":"2025-03-09T06:38:42.028984Z","shell.execute_reply":"2025-03-09T06:38:42.576691Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"trainer.train()\n\ntrainer.save_model(\"wav2vec2_final_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:38:42.578105Z","iopub.execute_input":"2025-03-09T06:38:42.578393Z","iopub.status.idle":"2025-03-09T07:56:08.373689Z","shell.execute_reply.started":"2025-03-09T06:38:42.578365Z","shell.execute_reply":"2025-03-09T07:56:08.372993Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='540' max='540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [540/540 1:17:14, Epoch 9/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Roc Auc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.666000</td>\n      <td>0.526728</td>\n      <td>0.858545</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.487400</td>\n      <td>0.262046</td>\n      <td>0.969418</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.316600</td>\n      <td>0.160775</td>\n      <td>0.981672</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.262700</td>\n      <td>0.128192</td>\n      <td>0.990316</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.188500</td>\n      <td>0.084634</td>\n      <td>0.995263</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.145900</td>\n      <td>0.076257</td>\n      <td>0.996134</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.123600</td>\n      <td>0.075953</td>\n      <td>0.994172</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.106900</td>\n      <td>0.107707</td>\n      <td>0.995668</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.085200</td>\n      <td>0.077966</td>\n      <td>0.996491</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"test = dataset['test']\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\"/kaggle/working/wav2vec2_final_model\")\nmodel = AutoModelForAudioClassification.from_pretrained(\"/kaggle/working/wav2vec2_final_model\")\n\n# Move model to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()  # Set model to evaluation mode","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:56:08.374529Z","iopub.execute_input":"2025-03-09T07:56:08.374760Z","iopub.status.idle":"2025-03-09T07:56:08.586008Z","shell.execute_reply.started":"2025-03-09T07:56:08.374738Z","shell.execute_reply":"2025-03-09T07:56:08.585350Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Wav2Vec2ForSequenceClassification(\n  (wav2vec2): Wav2Vec2Model(\n    (feature_extractor): Wav2Vec2FeatureEncoder(\n      (conv_layers): ModuleList(\n        (0): Wav2Vec2GroupNormConvLayer(\n          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n          (activation): GELUActivation()\n          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n        )\n        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n          (activation): GELUActivation()\n        )\n        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n          (activation): GELUActivation()\n        )\n      )\n    )\n    (feature_projection): Wav2Vec2FeatureProjection(\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (projection): Linear(in_features=512, out_features=768, bias=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): Wav2Vec2Encoder(\n      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n        (conv): ParametrizedConv1d(\n          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n          (parametrizations): ModuleDict(\n            (weight): ParametrizationList(\n              (0): _WeightNorm()\n            )\n          )\n        )\n        (padding): Wav2Vec2SamePadLayer()\n        (activation): GELUActivation()\n      )\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (layers): ModuleList(\n        (0-11): 12 x Wav2Vec2EncoderLayer(\n          (attention): Wav2Vec2SdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): Wav2Vec2FeedForward(\n            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (projector): Linear(in_features=768, out_features=256, bias=True)\n  (classifier): Linear(in_features=256, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"idx = []\nis_tts = []\n\nx = 1\nfor i in test:\n    idx.append(i['id'])\n\n    # Extract features\n    inputs = feature_extractor(i[\"audio\"][\"array\"], sampling_rate=16000, return_tensors=\"pt\")\n    \n    # Move inputs to GPU\n    inputs = {key: val.to(device) for key, val in inputs.items()}\n\n    with torch.no_grad():\n        logits = model(**inputs).logits\n\n    # Move logits back to CPU before converting to NumPy\n    softmax_values = torch.nn.functional.softmax(logits, dim=1)\n    softmax_second_element = softmax_values[0, 1].item()  # Convert to Python float\n\n    is_tts.append(softmax_second_element)\n\n    print(x,end=\"\\r\")\n    x += 1\n\n# Create dataframe and save to CSV\ndf = pd.DataFrame({\"id\": idx, \"is_tts\": is_tts})\n\ndf.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:56:08.586722Z","iopub.execute_input":"2025-03-09T07:56:08.586952Z","iopub.status.idle":"2025-03-09T07:58:38.320723Z","shell.execute_reply.started":"2025-03-09T07:56:08.586932Z","shell.execute_reply":"2025-03-09T07:58:38.319840Z"}},"outputs":[{"name":"stdout","text":"2635\r","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/working/submission.csv\")\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:58:38.321590Z","iopub.execute_input":"2025-03-09T07:58:38.321906Z","iopub.status.idle":"2025-03-09T07:58:38.350825Z","shell.execute_reply.started":"2025-03-09T07:58:38.321873Z","shell.execute_reply":"2025-03-09T07:58:38.350103Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                        id    is_tts\n0        ASM_F_ANGER_00109  0.003141\n1        ASM_F_ANGER_00127  0.997384\n2        ASM_F_ANGER_00386  0.003011\n3        ASM_F_ANGER_00103  0.997394\n4        ASM_F_ANGER_00434  0.003236\n...                    ...       ...\n2630  TAM_F_SURPRISE_00387  0.997373\n2631  TAM_F_SURPRISE_00173  0.997419\n2632  TAM_F_SURPRISE_00718  0.003096\n2633  TAM_F_SURPRISE_00127  0.003215\n2634  TAM_F_SURPRISE_00282  0.987839\n\n[2635 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>is_tts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ASM_F_ANGER_00109</td>\n      <td>0.003141</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ASM_F_ANGER_00127</td>\n      <td>0.997384</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ASM_F_ANGER_00386</td>\n      <td>0.003011</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ASM_F_ANGER_00103</td>\n      <td>0.997394</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ASM_F_ANGER_00434</td>\n      <td>0.003236</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2630</th>\n      <td>TAM_F_SURPRISE_00387</td>\n      <td>0.997373</td>\n    </tr>\n    <tr>\n      <th>2631</th>\n      <td>TAM_F_SURPRISE_00173</td>\n      <td>0.997419</td>\n    </tr>\n    <tr>\n      <th>2632</th>\n      <td>TAM_F_SURPRISE_00718</td>\n      <td>0.003096</td>\n    </tr>\n    <tr>\n      <th>2633</th>\n      <td>TAM_F_SURPRISE_00127</td>\n      <td>0.003215</td>\n    </tr>\n    <tr>\n      <th>2634</th>\n      <td>TAM_F_SURPRISE_00282</td>\n      <td>0.987839</td>\n    </tr>\n  </tbody>\n</table>\n<p>2635 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":13}]}